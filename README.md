<!-- # Awesome Embodied Category Discovery

A curated collection of recent works on **Embodied Category Discovery** and **Embodied Continual Learning** from top AI, ML, Computer Vision, and Robotics conferences.

## ðŸ“š Table of Contents

- [Overview](#overview)
- [Conferences Covered](#conferences-covered)
- [Papers by Year](#papers-by-year)
- [Contributing](#contributing)
- [Citation](#citation)

## ðŸŽ¯ Overview

This repository maintains a comprehensive collection of research papers related to:
- **Embodied Category Discovery**: Learning object categories through embodied interaction
- **Embodied Continual Learning**: Continuous learning in embodied AI systems
- **Robotic Perception**: Visual and multimodal perception for robots
- **Interactive Learning**: Learning through physical interaction and exploration

## ðŸ›ï¸ Conferences Covered

- **NeurIPS** - Neural Information Processing Systems
- **ICML** - International Conference on Machine Learning
- **ICLR** - International Conference on Learning Representations
- **CVPR** - Computer Vision and Pattern Recognition
- **ICCV** - International Conference on Computer Vision
- **ECCV** - European Conference on Computer Vision
- **ICRA** - International Conference on Robotics and Automation
- **IROS** - Intelligent Robots and Systems
- **CoRL** - Conference on Robot Learning
- **RSS** - Robotics: Science and Systems
- **AAAI** - Association for the Advancement of Artificial Intelligence
- **IJCAI** - International Joint Conference on Artificial Intelligence

## ðŸ“– Papers by Year

### 2025

#### arXiv 2025
- **""Body Discovery of Embodied AI""** - Zhe Sun, Pengfei Tian, Xiaozhu Hu, Xiaoyu Zhao, Huiying Li, Zhenliang Zhang1
  - ðŸ“„ [Paper](https://arxiv.org/pdf/2503.19941v1) | ðŸ’» [Code](link-to-code) | ðŸ”— [Project Page](link-to-project)



### 2024

#### NeurIPS 2024
- **[Paper Title]** - Authors  
  - ðŸ“„ [Paper](link-to-paper) | ðŸ’» [Code](link-to-code) | ðŸ”— [Project Page](link-to-project)

#### ICML 2024
- **[Paper Title]** - Authors  
  - ðŸ“„ [Paper](link-to-paper) | ðŸ’» [Code](link-to-code) | ðŸ”— [Project Page](link-to-project)

#### CVPR 2024
- **[Paper Title]** - Authors  
  - ðŸ“„ [Paper](link-to-paper) | ðŸ’» [Code](link-to-code) | ðŸ”— [Project Page](link-to-project)

#### ICRA 2024
- **[Paper Title]** - Authors  
  - ðŸ“„ [Paper](link-to-paper) | ðŸ’» [Code](link-to-code) | ðŸ”— [Project Page](link-to-project)

### 2023

#### NeurIPS 2023
- **[Paper Title]** - Authors  
  - ðŸ“„ [Paper](link-to-paper) | ðŸ’» [Code](link-to-code) | ðŸ”— [Project Page](link-to-project)

#### ICML 2023
- **[Paper Title]** - Authors  
  - ðŸ“„ [Paper](link-to-paper) | ðŸ’» [Code](link-to-code) | ðŸ”— [Project Page](link-to-project)

#### CVPR 2023
- **[Paper Title]** - Authors  
  - ðŸ“„ [Paper](link-to-paper) | ðŸ’» [Code](link-to-code) | ðŸ”— [Project Page](link-to-project)

#### ICRA 2023
- **[Paper Title]** - Authors  
  - ðŸ“„ [Paper](link-to-paper) | ðŸ’» [Code](link-to-code) | ðŸ”— [Project Page](link-to-project)

### 2022

#### NeurIPS 2022
- **[Paper Title]** - Authors  
  - ðŸ“„ [Paper](link-to-paper) | ðŸ’» [Code](link-to-code) | ðŸ”— [Project Page](link-to-project)

#### ICML 2022
- **[Paper Title]** - Authors  
  - ðŸ“„ [Paper](link-to-paper) | ðŸ’» [Code](link-to-code) | ðŸ”— [Project Page](link-to-project)

#### CVPR 2022
- **[Paper Title]** - Authors  
  - ðŸ“„ [Paper](link-to-paper) | ðŸ’» [Code](link-to-code) | ðŸ”— [Project Page](link-to-project)

#### ICRA 2022
- **[Paper Title]** - Authors  
  - ðŸ“„ [Paper](link-to-paper) | ðŸ’» [Code](link-to-code) | ðŸ”— [Project Page](link-to-project)

### 2021

#### NeurIPS 2021
- **[Paper Title]** - Authors  
  - ðŸ“„ [Paper](link-to-paper) | ðŸ’» [Code](link-to-code) | ðŸ”— [Project Page](link-to-project)

#### ICML 2021
- **[Paper Title]** - Authors  
  - ðŸ“„ [Paper](link-to-paper) | ðŸ’» [Code](link-to-code) | ðŸ”— [Project Page](link-to-project)

#### CVPR 2021
- **[Paper Title]** - Authors  
  - ðŸ“„ [Paper](link-to-paper) | ðŸ’» [Code](link-to-code) | ðŸ”— [Project Page](link-to-project)

#### ICRA 2021
- **[Paper Title]** - Authors  
  - ðŸ“„ [Paper](link-to-paper) | ðŸ’» [Code](link-to-code) | ðŸ”— [Project Page](link-to-project)

### 2020

#### NeurIPS 2020
- **[Paper Title]** - Authors  
  - ðŸ“„ [Paper](link-to-paper) | ðŸ’» [Code](link-to-code) | ðŸ”— [Project Page](link-to-project)

#### ICML 2020
- **[Paper Title]** - Authors  
  - ðŸ“„ [Paper](link-to-paper) | ðŸ’» [Code](link-to-code) | ðŸ”— [Project Page](link-to-project)

#### CVPR 2020
- **[Paper Title]** - Authors  
  - ðŸ“„ [Paper](link-to-paper) | ðŸ’» [Code](link-to-code) | ðŸ”— [Project Page](link-to-project)

#### ICRA 2020
- **[Paper Title]** - Authors  
  - ðŸ“„ [Paper](link-to-paper) | ðŸ’» [Code](link-to-code) | ðŸ”— [Project Page](link-to-project)

## ðŸš€ Contributing

We welcome contributions! To add a new paper:

1. **Fork** this repository
2. **Add** the paper in the appropriate year and conference section
3. **Follow** the format: `**[Paper Title]** - Authors`
4. **Include** links to paper, code, and project page if available
5. **Submit** a pull request

### Paper Entry Format

```markdown
- **[Paper Title]** - Authors  
  - ðŸ“„ [Paper](link-to-paper) | ðŸ’» [Code](link-to-code) | ðŸ”— [Project Page](link-to-project)
```

### Required Information

- **Paper Title**: Full title of the paper
- **Authors**: List of authors
- **Paper Link**: Direct link to the paper (arXiv, conference page, etc.)
- **Code Link**: Link to official implementation (GitHub, etc.) - optional
- **Project Page**: Link to project website/demo - optional

## ðŸ“ Citation

If you find this repository useful, please cite:

```bibtex
@misc{awesome-embodied-category-discovery,
  title={Awesome Embodied Category Discovery},
  author={Your Name},
  year={2024},
  url={https://github.com/yourusername/Awesome-Embodied-Category-Discovery}
}
```

## ðŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ðŸ¤ Acknowledgments

Thanks to all contributors and researchers in the embodied AI community for their valuable work.

---

**Note**: This is a living document. Papers are continuously added as new research emerges. Feel free to suggest papers or improvements! -->


# Awesome Embodied Category Discovery [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

A curated list of research where **robots or embodied agents navigate / explore environments** and simultaneously **discover new object categories / concepts** through unsupervised or self-supervised means.

---

## ðŸ“š Table of Contents

- [Overview](#overview)  
- [Papers by Year](#papers-by-year)  
  - [2020s](#2020s)  
  - [Earlier Works](#earlier-works)  
- [Datasets & Simulators](#datasets--simulators)  
- [Contributing](#contributing)  
- [Citation](#citation)  
- [License](#license)

---

## Overview

This list only includes papers that meet both criteria:

1. The method involves an **embodied agent / robot** that moves or observes over space/time.  
2. The method performs **novel category / object discovery** (clustering, modeling new object classes, etc.)

We do *not* include static-dataset novel class discovery works or methods that merely detect â€œunknownsâ€ without building new categories.

---

## Papers by Year

### 2020s

- **SCIM: Simultaneous Clustering, Inference, and Mapping for Open-World Semantic Scene Understanding**  
  *H. Blum, M. G. MÃ¼ller, A. Gawel, R. Siegwart, C. Cadena*  
  - ðŸ“„ [Paper](https://arxiv.org/abs/2206.10670)  [oai_citation:0â€¡arXiv](https://arxiv.org/abs/2206.10670?utm_source=chatgpt.com)  
  - ðŸ’» [Code](https://github.com/hermannsblum/scim)  
  > Robot explores unknown indoor environments, clusters novel semantic classes during mapping, and uses these clusters to self-supervise improvement of semantic segmentation.

---

### Earlier Works

- **Unsupervised Discovery of Object Classes with a Mobile Robot**  
  *J. Mason, B. Marthi, R. Parr*  
  - ðŸ“„ [Paper](https://users.cs.duke.edu/~parr/icra14_mmp.pdf)  [oai_citation:1â€¡Duke University Computer Science](https://users.cs.duke.edu/~parr/icra14_mmp.pdf?utm_source=chatgpt.com)  
  > A mobile robot system that discovers object candidates and clusters them into novel object classes during exploration.

- **Unsupervised Discovery of Object Classes from Range Data using Latent Dirichlet Allocation**  
  *F. Endres, C. Plagemann, C. Stachniss, W. Burgard*  
  - ðŸ“„ [Paper](https://www.roboticsproceedings.org/rss05/p15.pdf)  [oai_citation:2â€¡roboticsproceedings.org](https://www.roboticsproceedings.org/rss05/p15.pdf?utm_source=chatgpt.com)  
  > Robot uses 3D range (laser) data and LDA to cluster objects into classes without prior knowledge, discovering new object categories from sensor observations.

- **Simultaneous Localization, Mapping, and Manipulation for Unsupervised Object Discovery**  
  *Lu Ma, Mahsa Ghafarianzadeh, Dave Coleman, Nikolaus Correll, Gabe Sibley*  
  - ðŸ“„ [Paper](https://arxiv.org/abs/1411.0802)  
  > Combines SLAM and object discovery: the robot builds maps while identifying new object candidates and refining their models over time via appearance and manipulation cues.

---

## Datasets & Simulators

These are useful platforms for running experiments in embodied navigation and perception:

- **Habitat-Matterport 3D (HM3D / HM3D-Semantics)** â€” real indoor scans for navigation + semantic labeling  
- **Replica / ReplicaCAD** â€” scanned indoor scenes with class / instance labels  
- **iGibson / Gibson** â€” simulated indoor environments with navigation + object interaction  
- **AI2-THOR / ProcTHOR** â€” interactive household environments supporting movement + manipulation  
- **JRDB** â€” egocentric robot video dataset (robot motion + perception)  
- **NovelCraft** â€” simulation environment designed for novelty detection / category discovery with an active agent  

---

## Contributing

To contribute a new embodied discovery paper:

1. Fork the repository  
2. Add the paper to the appropriate year under **Papers by Year**  
3. Use this template:

    ```markdown
    - **[Paper Title]** â€” Authors (Venue, Year)  
      - ðŸ“„ [Paper](link) | ðŸ’» [Code](link)  
      > One-line summary: how it performs embodied category discovery  
    ```

4. Submit a Pull Request  

---

## Citation

```bibtex
@misc{awesome-embodied-category-discovery,
  title = {Awesome Embodied Category Discovery},
  author = {Your Name},
  year = {2025},
  url = {https://github.com/yourusername/Awesome-Embodied-Category-Discovery}
}