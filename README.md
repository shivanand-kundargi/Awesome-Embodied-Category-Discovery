<!-- # Awesome Embodied Category Discovery

A curated collection of recent works on **Embodied Category Discovery** and **Embodied Continual Learning** from top AI, ML, Computer Vision, and Robotics conferences.

## 📚 Table of Contents

- [Overview](#overview)
- [Conferences Covered](#conferences-covered)
- [Papers by Year](#papers-by-year)
- [Contributing](#contributing)
- [Citation](#citation)

## 🎯 Overview

This repository maintains a comprehensive collection of research papers related to:
- **Embodied Category Discovery**: Learning object categories through embodied interaction
- **Embodied Continual Learning**: Continuous learning in embodied AI systems
- **Robotic Perception**: Visual and multimodal perception for robots
- **Interactive Learning**: Learning through physical interaction and exploration

## 🏛️ Conferences Covered

- **NeurIPS** - Neural Information Processing Systems
- **ICML** - International Conference on Machine Learning
- **ICLR** - International Conference on Learning Representations
- **CVPR** - Computer Vision and Pattern Recognition
- **ICCV** - International Conference on Computer Vision
- **ECCV** - European Conference on Computer Vision
- **ICRA** - International Conference on Robotics and Automation
- **IROS** - Intelligent Robots and Systems
- **CoRL** - Conference on Robot Learning
- **RSS** - Robotics: Science and Systems
- **AAAI** - Association for the Advancement of Artificial Intelligence
- **IJCAI** - International Joint Conference on Artificial Intelligence

## 📖 Papers by Year

### 2025

#### arXiv 2025
- **""Body Discovery of Embodied AI""** - Zhe Sun, Pengfei Tian, Xiaozhu Hu, Xiaoyu Zhao, Huiying Li, Zhenliang Zhang1
  - 📄 [Paper](https://arxiv.org/pdf/2503.19941v1) | 💻 [Code](link-to-code) | 🔗 [Project Page](link-to-project)



### 2024

#### NeurIPS 2024
- **[Paper Title]** - Authors  
  - 📄 [Paper](link-to-paper) | 💻 [Code](link-to-code) | 🔗 [Project Page](link-to-project)

#### ICML 2024
- **[Paper Title]** - Authors  
  - 📄 [Paper](link-to-paper) | 💻 [Code](link-to-code) | 🔗 [Project Page](link-to-project)

#### CVPR 2024
- **[Paper Title]** - Authors  
  - 📄 [Paper](link-to-paper) | 💻 [Code](link-to-code) | 🔗 [Project Page](link-to-project)

#### ICRA 2024
- **[Paper Title]** - Authors  
  - 📄 [Paper](link-to-paper) | 💻 [Code](link-to-code) | 🔗 [Project Page](link-to-project)

### 2023

#### NeurIPS 2023
- **[Paper Title]** - Authors  
  - 📄 [Paper](link-to-paper) | 💻 [Code](link-to-code) | 🔗 [Project Page](link-to-project)

#### ICML 2023
- **[Paper Title]** - Authors  
  - 📄 [Paper](link-to-paper) | 💻 [Code](link-to-code) | 🔗 [Project Page](link-to-project)

#### CVPR 2023
- **[Paper Title]** - Authors  
  - 📄 [Paper](link-to-paper) | 💻 [Code](link-to-code) | 🔗 [Project Page](link-to-project)

#### ICRA 2023
- **[Paper Title]** - Authors  
  - 📄 [Paper](link-to-paper) | 💻 [Code](link-to-code) | 🔗 [Project Page](link-to-project)

### 2022

#### NeurIPS 2022
- **[Paper Title]** - Authors  
  - 📄 [Paper](link-to-paper) | 💻 [Code](link-to-code) | 🔗 [Project Page](link-to-project)

#### ICML 2022
- **[Paper Title]** - Authors  
  - 📄 [Paper](link-to-paper) | 💻 [Code](link-to-code) | 🔗 [Project Page](link-to-project)

#### CVPR 2022
- **[Paper Title]** - Authors  
  - 📄 [Paper](link-to-paper) | 💻 [Code](link-to-code) | 🔗 [Project Page](link-to-project)

#### ICRA 2022
- **[Paper Title]** - Authors  
  - 📄 [Paper](link-to-paper) | 💻 [Code](link-to-code) | 🔗 [Project Page](link-to-project)

### 2021

#### NeurIPS 2021
- **[Paper Title]** - Authors  
  - 📄 [Paper](link-to-paper) | 💻 [Code](link-to-code) | 🔗 [Project Page](link-to-project)

#### ICML 2021
- **[Paper Title]** - Authors  
  - 📄 [Paper](link-to-paper) | 💻 [Code](link-to-code) | 🔗 [Project Page](link-to-project)

#### CVPR 2021
- **[Paper Title]** - Authors  
  - 📄 [Paper](link-to-paper) | 💻 [Code](link-to-code) | 🔗 [Project Page](link-to-project)

#### ICRA 2021
- **[Paper Title]** - Authors  
  - 📄 [Paper](link-to-paper) | 💻 [Code](link-to-code) | 🔗 [Project Page](link-to-project)

### 2020

#### NeurIPS 2020
- **[Paper Title]** - Authors  
  - 📄 [Paper](link-to-paper) | 💻 [Code](link-to-code) | 🔗 [Project Page](link-to-project)

#### ICML 2020
- **[Paper Title]** - Authors  
  - 📄 [Paper](link-to-paper) | 💻 [Code](link-to-code) | 🔗 [Project Page](link-to-project)

#### CVPR 2020
- **[Paper Title]** - Authors  
  - 📄 [Paper](link-to-paper) | 💻 [Code](link-to-code) | 🔗 [Project Page](link-to-project)

#### ICRA 2020
- **[Paper Title]** - Authors  
  - 📄 [Paper](link-to-paper) | 💻 [Code](link-to-code) | 🔗 [Project Page](link-to-project)

## 🚀 Contributing

We welcome contributions! To add a new paper:

1. **Fork** this repository
2. **Add** the paper in the appropriate year and conference section
3. **Follow** the format: `**[Paper Title]** - Authors`
4. **Include** links to paper, code, and project page if available
5. **Submit** a pull request

### Paper Entry Format

```markdown
- **[Paper Title]** - Authors  
  - 📄 [Paper](link-to-paper) | 💻 [Code](link-to-code) | 🔗 [Project Page](link-to-project)
```

### Required Information

- **Paper Title**: Full title of the paper
- **Authors**: List of authors
- **Paper Link**: Direct link to the paper (arXiv, conference page, etc.)
- **Code Link**: Link to official implementation (GitHub, etc.) - optional
- **Project Page**: Link to project website/demo - optional

## 📝 Citation

If you find this repository useful, please cite:

```bibtex
@misc{awesome-embodied-category-discovery,
  title={Awesome Embodied Category Discovery},
  author={Your Name},
  year={2024},
  url={https://github.com/yourusername/Awesome-Embodied-Category-Discovery}
}
```

## 📄 License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## 🤝 Acknowledgments

Thanks to all contributors and researchers in the embodied AI community for their valuable work.

---

**Note**: This is a living document. Papers are continuously added as new research emerges. Feel free to suggest papers or improvements! -->

# Awesome Embodied Category Discovery [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

A curated list of research and resources on **Embodied Category / Concept Discovery**, where agents or robots **navigate / explore environments** and **discover new object categories / semantics**.

---

## 📚 Table of Contents

- [Overview](#overview)  
- [Papers by Year](#papers-by-year)  
  - [2020s](#2020s)  
  - [Earlier Works](#earlier-works)  
- [Datasets & Simulators](#datasets--simulators)  
- [Contributing](#contributing)  
- [Citation](#citation)  
- [License](#license)

---

## Overview

We include **only** papers that combine:

1. **Embodied exploration / navigation** (the agent is moving/observing through an environment), and  
2. **Category / object / concept discovery** (i.e. clustering or discovering new semantic classes), not just detecting unknowns.

Works purely on static datasets, or only on unknown-detection without discovering categories, are excluded.

---

## Papers by Year

### 2020s

- **SCIM: Simultaneous Clustering, Inference, and Mapping for Open-World Semantic Scene Understanding**  
  *Hermann Blum, Marcus G. Müller, Abel Gawel, Roland Siegwart, Cesar Cadena*  
  - 📄 [Paper (arXiv)](https://arxiv.org/abs/2206.10670)  
  - 💻 [Code](https://github.com/hermannsblum/scim)  
  > The robot explores unknown indoor environments, clusters novel semantic classes online while building a map, and uses these clusters to self-supervise semantic segmentation.    

- **SCOD: Active Object Detection for Embodied Agents using Sensory Commutativity of Action Sequences**  
  *Hugo Caselles-Dupré, Michael Garcia-Ortiz, David Filliat* (2021)  
  - 📄 [Paper (arXiv)](https://arxiv.org/abs/2107.02069)  
  > Uses the commutativity of action sequences to help an embodied agent actively discover objects.   

---

### Earlier Works

- **Unsupervised Discovery of Object Classes with a Mobile Robot**  
  *Julian Mason, Bhaskara Marthi, Ronald Parr* (ICRA 2014)  
  - 📄 [Paper](https://users.cs.duke.edu/~parr/icra14_mmp.pdf)  
  > A mobile robot collects sensory data and clusters object proposals into classes during exploration.

- **Simultaneous Localization, Mapping, and Manipulation for Unsupervised Object Discovery**  
  *Lu Ma, Mahsa Ghafarianzadeh, Dave Coleman, Nikolaus Correll, Gabe Sibley*  
  - 📄 [Paper (arXiv)](https://arxiv.org/abs/1411.0802)  
  > Combines SLAM and unsupervised object discovery; uses appearance / motion / manipulation cues to refine discovered object models.

---

## Datasets & Simulators

Here are environment datasets and simulation platforms frequently used for embodied AI / navigation research.

- **Habitat-Matterport 3D (HM3D)** — high-fidelity indoor scans for embodied agents  
  - 📄 [HM3D paper (arXiv)](https://arxiv.org/abs/2109.08238)  [oai_citation:2‡arXiv](https://arxiv.org/abs/2109.08238?utm_source=chatgpt.com)  
  - 🏷️ [GitHub / dataset repo](https://github.com/facebookresearch/habitat-matterport3d-dataset)   

- **HM3D-Semantics (HM3D-Sem)** — semantic annotations over HM3D scenes  
  - 📄 [HM3D-Sem Dataset details](https://aihabitat.org/datasets/hm3d-semantics/)    
  - Poster / project page: [CVPR Poster](https://cvpr.thecvf.com/virtual/2023/poster/22424)  

- **ReplicaCAD** — rearrangeable indoor scenes for interaction / simulation  
  - 📄 [ReplicaCAD Dataset (Habitat)](https://aihabitat.org/datasets/replica_cad/)  

- **HSSD / Habitat Synthetic Scenes Dataset**  
  - Repo / usage: [3dlg-hcvc/hssd](https://github.com/3dlg-hcvc/hssd)  

- **AI2-THOR (Habitat-compatible version)**  
  - Dataset page: [AI2THOR-Hab on Hugging Face](https://huggingface.co/datasets/hssd/ai2thor-hab)  
- **Joint 2D-3D Semantic Data for Indoor Scenes**  
  - Dataset / paper: [Armeni et al., Joint 2D-3D Semantic Data (arXiv)](https://arxiv.org/abs/1702.01105)  
  > Provides aligned RGB, depth, semantic and 3D representations — useful as auxiliary data source.

---

## Contributing

If you know a paper that truly does **embodied navigation + category discovery**, please:

1. Fork this repository  
2. Add the paper under the correct year  
3. Use this format:

    ```markdown
    - **[Paper Title]** — Authors (Venue, Year)  
      - 📄 [Paper](link) | 💻 [Code](link)  
      > One-line note: how it performs discovery + navigation  
    ```

4. Submit a Pull Request

---

## Citation

```bibtex
@misc{awesome-embodied-category-discovery,
  title = {Awesome Embodied Category Discovery},
  author = {Your Name},
  year = {2025},
  url = {https://github.com/yourusername/Awesome-Embodied-Category-Discovery}
}